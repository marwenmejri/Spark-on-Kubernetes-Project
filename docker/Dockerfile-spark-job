FROM openjdk:11-jre-slim

# Set the Spark version
ENV SPARK_VERSION=3.3.1
ENV HADOOP_VERSION=3
ENV DELTA_VERSION=1.2.1
ENV SPARK_HOME=/opt/spark

RUN apt-get update && \
    apt-get install -y wget procps

# Download and install Spark with Hadoop
RUN mkdir -p "${SPARK_HOME}" && \
    wget --no-verbose "https://archive.apache.org/dist/spark/spark-$SPARK_VERSION/spark-$SPARK_VERSION-bin-hadoop$HADOOP_VERSION.tgz" -O /tmp/spark.tgz && \
    tar -xvf /tmp/spark.tgz -C "${SPARK_HOME}" --strip-components=1 && \
    rm /tmp/spark.tgz

ENV PATH="${SPARK_HOME}/bin:${PATH}"

# Working directory
WORKDIR /app

# Install Python 3 and pip
RUN apt-get install -y python3 python3-pip && \
    update-alternatives --install "/usr/bin/python" "python" "$(which python3)" 1


COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 4040

CMD spark-submit \
    --conf spark.eventLog.enabled=true \
    --conf spark.eventLog.dir=file:/app/Logs/spark-logs \
    --packages io.delta:delta-core_2.12:$DELTA_VERSION --master local[*] ./src/spark_job.py
